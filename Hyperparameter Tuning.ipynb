{"cells":[{"cell_type":"code","execution_count":null,"id":"46b886da","metadata":{"id":"46b886da"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import RandomizedSearchCV"]},{"cell_type":"markdown","id":"cbed34cd","metadata":{"id":"cbed34cd"},"source":["# Process data"]},{"cell_type":"code","execution_count":null,"id":"8cf4b552","metadata":{"id":"8cf4b552"},"outputs":[],"source":["def get_data():\n","    test_dfn = pd.read_csv(r'test_df_after_fs_100.csv')\n","    #test_dfn = test_dfn.drop(['id'], axis=1)\n","    train_dfn = pd.read_csv(r'train_df_after_fs_100.csv')\n","    #train_dfn = train_dfn.drop(['id'], axis=1)\n","\n","    X_train_df = train_dfn.iloc[:, :train_dfn.shape[1]-1]\n","    X_test_df = test_dfn.iloc[:, :test_dfn.shape[1]-1]\n","\n","    Y_train_df = train_dfn.iloc[:, -1:]\n","    Y_test_df = test_dfn.iloc[:, -1:]\n","\n","\n","    return X_train_df, Y_train_df, X_test_df, Y_test_df\n","\n","\n","X_train_df, Y_train_df, X_test_df, Y_test_df = get_data()"]},{"cell_type":"code","execution_count":null,"id":"ef996fb7","metadata":{"id":"ef996fb7"},"outputs":[],"source":["pip install optuna"]},{"cell_type":"code","execution_count":null,"id":"982db8b6","metadata":{"id":"982db8b6"},"outputs":[],"source":["from sklearn import metrics\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"6060e64f","metadata":{"id":"6060e64f"},"source":["# KNN Tuning"]},{"cell_type":"code","execution_count":null,"id":"cc0f037b","metadata":{"id":"cc0f037b"},"outputs":[],"source":["import optuna\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","\n","\n","\n","def knn_objective(trial):\n","\n","    optimizer = trial.suggest_categorical('algorithm', ['auto','ball_tree','kd_tree','brute'])\n","    rf_max_depth = trial.suggest_int(\"k_n_neighbors\", 2, 10, log=True)\n","    weights = trial.suggest_categorical(\"weights\", [\"uniform\",  \"distance\"])\n","    p = trial.suggest_categorical(\"p\", [1, 2])\n","    leaf_size = trial.suggest_int(\"leaf_size\", 2, 16, step=4),\n","\n","\n","    clf = KNeighborsClassifier(n_neighbors=rf_max_depth,algorithm=optimizer, weights = weights, p = p, n_jobs = -1)\n","    clf.fit(X_train_df, Y_train_df)\n","\n","    preds = clf.predict(X_test_df)\n","    pred_labels = np.rint(preds)\n","    accuracy = metrics.accuracy_score(Y_test_df, pred_labels)\n","    accuracy\n","\n","    return accuracy\n","\n","if __name__ == \"__main__\":\n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(knn_objective, n_trials=128, timeout=360000)\n","\n","    print(\"Number of finished trials: \", len(study.trials))\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: {}\".format(trial.value))\n","    print(\"  Params: \")\n","    for key, value in trial.params.items():\n","        print(\"    {}: {}\".format(key, value))"]},{"cell_type":"markdown","id":"7c2c44a2","metadata":{"id":"7c2c44a2"},"source":["# xgboost tuning"]},{"cell_type":"code","execution_count":null,"id":"100fe08a","metadata":{"id":"100fe08a"},"outputs":[],"source":["\"\"\"\n","Optuna example that optimizes a classifier configuration for cancer dataset\n","using XGBoost.\n","In this example, we optimize the validation accuracy of cancer detection\n","using XGBoost. We optimize both the choice of booster model and its\n","hyperparameters.\n","\"\"\"\n","\n","import numpy as np\n","import optuna\n","\n","import sklearn.datasets\n","import sklearn.metrics\n","from sklearn.model_selection import train_test_split\n","import xgboost as xgb\n","\n","\n","def objective(trial):\n","\n","    dtrain = xgb.DMatrix(X_train_df, label=Y_train_df)\n","    dvalid = xgb.DMatrix(X_test_df, label=Y_test_df)\n","\n","    param = {\n","        \"verbosity\": 0,\n","        \"objective\": \"multi:softmax\",\n","        # use exact for small dataset.\n","        #\"tree_method\": \"exact\",\n","        \"num_class\" : 10,\n","        # defines booster, gblinear for linear functions.\n","        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n","        # L2 regularization weight.\n","        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n","        # L1 regularization weight.\n","        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n","        # sampling ratio for training data.\n","        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n","        # sampling according to each tree.\n","        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n","        \"early_stopping_rounds\": trial.suggest_int(\"early_stopping_rounds\", 8, 32),\n","        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [16, 32, 64, 96]),\n","    }\n","\n","    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n","        # maximum depth of the tree, signifies complexity of the tree.\n","        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n","        # minimum child weight, larger the term more conservative the tree.\n","        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n","        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n","        # defines how selective algorithm is.\n","        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n","        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n","\n","    if param[\"booster\"] == \"dart\":\n","        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n","        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n","        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n","        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n","\n","    bst = xgb.train(param, dtrain)\n","    preds = bst.predict(dvalid)\n","    pred_labels = np.rint(preds)\n","    accuracy = sklearn.metrics.accuracy_score(Y_test_df, pred_labels)\n","    return accuracy\n","\n","\n","if __name__ == \"__main__\":\n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(objective, n_trials=128, timeout=360000)\n","\n","    print(\"Number of finished trials: \", len(study.trials))\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: {}\".format(trial.value))\n","    print(\"  Params: \")\n","    for key, value in trial.params.items():\n","        print(\"    {}: {}\".format(key, value))"]},{"cell_type":"markdown","id":"2fa2d3cc","metadata":{"id":"2fa2d3cc"},"source":["# Random Forest"]},{"cell_type":"code","execution_count":null,"id":"313bb4ce","metadata":{"id":"313bb4ce"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","def rf_objective(trial:optuna.trial.Trial):\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 8, 128),\n","        'max_depth': trial.suggest_int('max_depth', 4, 16),\n","        'min_samples_split': trial.suggest_int('min_samples_split', 4, 32),\n","        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 16),\n","    }\n","\n","    clf = RandomForestClassifier(random_state=42, **params)\n","\n","    clf.fit(X_train_df, Y_train_df)\n","\n","    preds = clf.predict(X_test_df)\n","    pred_labels = np.rint(preds)\n","    accuracy = sklearn.metrics.accuracy_score(Y_test_df, pred_labels)\n","    accuracy\n","\n","    return accuracy\n","\n","if __name__ == \"__main__\":\n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(rf_objective, n_trials=128, timeout=360000)\n","\n","    print(\"Number of finished trials: \", len(study.trials))\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: {}\".format(trial.value))\n","    print(\"  Params: \")\n","    for key, value in trial.params.items():\n","        print(\"    {}: {}\".format(key, value))"]},{"cell_type":"markdown","id":"06ed1adf","metadata":{"id":"06ed1adf"},"source":["# Decision Tree"]},{"cell_type":"code","execution_count":null,"id":"91958375","metadata":{"id":"91958375"},"outputs":[],"source":["import sklearn\n","print(sklearn.__version__)"]},{"cell_type":"code","execution_count":null,"id":"0cf40a01","metadata":{"id":"0cf40a01"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","\n","def dt_objective(trial:optuna.trial.Trial):\n","    params = {\n","        \"splitter\" : trial.suggest_categorical(\"splitter\", [\"best\",\"random\"]),\n","        \"criterion\" : trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n","        \"max_depth\" : trial.suggest_int('max_depth', 4, 16),\n","        \"min_samples_split\" : trial.suggest_int('min_samples_split', 4, 32),\n","        \"min_samples_leaf\" : trial.suggest_int('min_samples_leaf', 2, 16),\n","        \"min_weight_fraction_leaf\" : trial.suggest_float(\"min_weight_fraction_leaf\", 1e-8, 0.5, log=True),\n","        \"min_impurity_decrease\" : trial.suggest_float(\"min_impurity_decrease\", 1e-8, 1.0, log=True),\n","        #\"max_features\" : trial.suggest_categorical(\"max_features\", [int, float, \"auto\", \"sqrt\", \"log2\"]),\n","        \"max_leaf_nodes\" : trial.suggest_int('max_leaf_nodes', 4, 32)\n","    }\n","\n","    clf = DecisionTreeClassifier(**params)\n","\n","\n","    clf.fit(X_train_df, Y_train_df)\n","\n","    preds = clf.predict(X_test_df)\n","    pred_labels = np.rint(preds)\n","    accuracy = metrics.accuracy_score(Y_test_df, pred_labels)\n","    accuracy\n","\n","    return accuracy\n","\n","if __name__ == \"__main__\":\n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(dt_objective, n_trials=128, timeout=360000)\n","\n","    print(\"Number of finished trials: \", len(study.trials))\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: {}\".format(trial.value))\n","    print(\"  Params: \")\n","    for key, value in trial.params.items():\n","        print(\"    {}: {}\".format(key, value))"]},{"cell_type":"markdown","id":"51bbc669","metadata":{"id":"51bbc669"},"source":["# SVM"]},{"cell_type":"code","execution_count":null,"id":"0f0cdf29","metadata":{"id":"0f0cdf29"},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","\n","def svm_objective(trial):\n","    # C\n","    svc_c = trial.suggest_loguniform('C', 1e0, 1e2)\n","    # kernel\n","    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n","    # SVC\n","    clf = SVC(C=svc_c, kernel=kernel)\n","    clf.fit(X_train_df, Y_train_df)\n","\n","    preds = clf.predict(X_test_df)\n","    pred_labels = np.rint(preds)\n","    accuracy = sklearn.metrics.accuracy_score(Y_test_df, pred_labels)\n","    accuracy\n","\n","    return accuracy\n","\n","if __name__ == \"__main__\":\n","    study = optuna.create_study(direction=\"maximize\")\n","    study.optimize(svm_objective, n_trials=128, timeout=36000000)\n","\n","    print(\"Number of finished trials: \", len(study.trials))\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: {}\".format(trial.value))\n","    print(\"  Params: \")\n","    for key, value in trial.params.items():\n","        print(\"    {}: {}\".format(key, value))"]},{"cell_type":"code","execution_count":null,"id":"f29730aa","metadata":{"id":"f29730aa"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"add465da","metadata":{"id":"add465da"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}