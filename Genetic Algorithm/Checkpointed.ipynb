{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a6d7df",
   "metadata": {},
   "source": [
    "# Why do we need Seeding:\n",
    "\n",
    "https://stackoverflow.com/questions/21494489/what-does-numpy-random-seed0-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3091780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  363144\n",
      "Number of testing samples:  82332\n",
      "(363144, 44)\n",
      "(363144, 1)\n",
      "(82332, 44)\n",
      "(82332, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from scipy.stats import pearsonr\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2,f_classif\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(r'test_dataset.csv')\n",
    "train_df = pd.read_csv(r'balanced_train_dataset.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label = ['Analysis','Backdoor','DoS','Exploits','Fuzzers','Generic','Normal','Reconnaissance','Shellcode','Worms']\n",
    "\n",
    "X_train = train_df.iloc[:,:train_df.shape[1]-10]\n",
    "Y_train = train_df.iloc[:,-10:].to_numpy()\n",
    "\n",
    "X_test = test_df.iloc[:,:test_df.shape[1]-10]\n",
    "Y_test = test_df.iloc[:,-10:].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "def LabelAttack(Y):\n",
    "  attack = []\n",
    "  for i in range(Y.shape[0]):\n",
    "    k = 0\n",
    "    for j in range(Y.shape[1]):\n",
    "      if(Y[i][j] == 1):\n",
    "        k = j\n",
    "        break\n",
    "    attack.append(k)\n",
    "  return attack\n",
    "\n",
    "\n",
    "Y_train = np.asarray(LabelAttack(Y_train)).reshape(-1,1)\n",
    "Y_test = np.asarray(LabelAttack(Y_test)).reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of training samples: \",train_df.shape[0])\n",
    "print(\"Number of testing samples: \",test_df.shape[0])\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a546a",
   "metadata": {},
   "source": [
    "# Maine alag se likha hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eab0b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    return XGBClassifier(learning_rate = 0.1,\n",
    "          n_estimators=8, \n",
    "          max_depth=3,\n",
    "          nthread=4,\n",
    "          seed = 1)\n",
    "\n",
    "def save_generation(gen_id, constant, data):\n",
    "    with open(str(gen_id) + \"_\" + str(constant) + \"_.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "def load_generation(gen_id, constant):\n",
    "    with open(str(gen_id) + \"_\" + str(constant) + \"_.pickle\", 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1b4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def AnovaScores(X_train, y_train):\n",
    "\t# configure to select all features\n",
    "\tfs = SelectKBest(score_func=f_classif, k='all')\n",
    "\t# learn relationship from training data\n",
    "\tfs.fit(X_train, y_train)\n",
    "\t# transform train input data\n",
    "\tX_train_fs = fs.transform(X_train)\n",
    "\treturn fs.scores_\n",
    "\n",
    "\n",
    "score = AnovaScores(X_train,Y_train)\n",
    "score = np.asarray(score).reshape(len(score),-1)\n",
    "\n",
    "#Scale AnovaScore\n",
    "mimxScale = MinMaxScaler()\n",
    "score = mimxScale.fit_transform(score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def countNoOfFeatures(chromosome):\n",
    "    k = 0\n",
    "    for gene in chromosome:\n",
    "      k += gene\n",
    "    return k\n",
    "\n",
    "def computeMerit(chromosome):\n",
    "    print(\"computing merit:\")\n",
    "    k = countNoOfFeatures(chromosome)\n",
    "    x = X_train.iloc[:,chromosome].to_numpy()\n",
    "    y = Y_train\n",
    "\n",
    "    n = x.shape[1]\n",
    "    Rcf = 0\n",
    "    Rff = 0\n",
    "    for i in range(n):\n",
    "      for j in range(i+1,n):\n",
    "        Rff += pearsonr(x[:,i],x[:,j])[0]\n",
    "    \n",
    "    Rff = Rff/((n*(n-1))/2)\n",
    "    Rcf = 0.0\n",
    "    for i in range(score.shape[0]):\n",
    "      if(chromosome[i]==1):\n",
    "        Rcf += score[i][0]\n",
    "    Rcf = Rcf/n\n",
    "\n",
    "    print(\"Rcf: \",Rcf,\"/\",\"Rff: \",Rff)\n",
    "\n",
    "    merit = (k*Rcf)/sqrt(k+k*(k-1)*abs(Rff))\n",
    "    return merit\n",
    "\n",
    "\n",
    "def GMean(y_pred,y_true):\n",
    "    TP,TN,FP,FN = 0,0,0,0\n",
    "    cnf_matrix = confusion_matrix(y_true,y_pred)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    sensitivity = TP/(TP+FN) #true positive rate\n",
    "    specificity = TN/(TN+FP) #true negative rate\n",
    "    \n",
    "    g_mean = sqrt((sum(sensitivity)/len(sensitivity)) * (sum(specificity)/len(specificity)))\n",
    "    return g_mean\n",
    "\n",
    "\n",
    "def initilization_of_population(size,n_feat):\n",
    "    population = []\n",
    "    for i in range(size):\n",
    "        chromosome = np.ones(n_feat,dtype=np.bool)     \n",
    "        chromosome[:int(0.3*n_feat+0.5)]=False             \n",
    "        np.random.shuffle(chromosome)\n",
    "        population.append(chromosome)\n",
    "    return population\n",
    "\n",
    "def fitness_score(population, generation, a):\n",
    "    scores = []\n",
    "   \n",
    "\n",
    "    #varibale declare kiya\n",
    "    merits = []\n",
    "    g_means = []\n",
    "    f_scores = []\n",
    "    accs = []\n",
    "    \n",
    "    \n",
    "    for chromosome in population:\n",
    "        merit = computeMerit(chromosome)\n",
    "        model.fit(X_train.iloc[:,chromosome],Y_train)         \n",
    "        predictions = model.predict(X_test.iloc[:,chromosome])\n",
    "        g_mean = GMean(predictions,Y_test)\n",
    "        f_score = (a * merit) + ((1-a)*g_mean)\n",
    "        acc = accuracy_score(Y_test,predictions)\n",
    "\n",
    "        scores.append(f_score)\n",
    "\n",
    "        #variable me value append kari\n",
    "        merits.append(merit)\n",
    "        g_means.append(g_mean)\n",
    "        f_scores.append(f_score)\n",
    "        accs.append(acc)\n",
    "\n",
    "\n",
    "    #variable se dictionry me convert kiya\n",
    "    fitness_metrics = {\n",
    "          'merit' : merits,\n",
    "           'g_mean' : g_means,\n",
    "           'f_score' : f_scores,\n",
    "           'acc' : accs\n",
    "    }\n",
    "    \n",
    "\n",
    "    scores, population = np.array(scores), np.array(population) \n",
    "    inds = np.argsort(scores)                                             ###sorting in ascending order\n",
    "    \n",
    "    #also returning fitness matrics\n",
    "    return list(scores[inds][::-1]), list(population[inds,:][::-1]), fitness_metrics \n",
    "\n",
    "\n",
    "def selection(pop_after_fit,selection_rate,pop_size):\n",
    "    population_nextgen = []\n",
    "    count = int(selection_rate*pop_size+0.5)                     \n",
    "    for i in range(count):\n",
    "        population_nextgen.append(pop_after_fit[i])\n",
    "    return population_nextgen\n",
    "    \n",
    "\n",
    "def crossover(pop_after_sel,selection_rate,pop_size):\n",
    "    print(\"Crossover operation: \")\n",
    "    pop_nextgen = pop_after_sel\n",
    "    count = (int((1-selection_rate)*pop_size + 0.5))//2\n",
    "    for i in range(0,count,2):\n",
    "        new_par1 = []\n",
    "        new_par2 = []\n",
    "        child_1 , child_2 = pop_nextgen[i] , pop_nextgen[i+1]\n",
    "        new_par1 = np.concatenate((child_1[:len(child_1)//2],child_2[len(child_1)//2:]))\n",
    "        new_par2 = np.concatenate((child_1[len(child_1)//2:],child_2[:len(child_1)//2]))\n",
    "        pop_nextgen.append(new_par1)\n",
    "        pop_nextgen.append(new_par2)\n",
    "    return pop_nextgen\n",
    "\n",
    "\n",
    "def mutation(pop_after_cross,mutation_rate,n_feat):   \n",
    "    print(\"Mutation operation: \")\n",
    "    mutation_range = int(mutation_rate*n_feat+0.5)\n",
    "    pop_next_gen = []\n",
    "    for n in range(0,len(pop_after_cross)):\n",
    "        chromo = pop_after_cross[n]\n",
    "        rand_posi = [] \n",
    "        for i in range(0,mutation_range):\n",
    "            #pos = randint(0,n_feat-1)\n",
    "            pos = np.random.randint(n_feat-1, size=1)[0]\n",
    "            \n",
    "            rand_posi.append(pos)\n",
    "        for j in rand_posi:\n",
    "            chromo[j] = not chromo[j]  \n",
    "        pop_next_gen.append(chromo)\n",
    "    return pop_next_gen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a245b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generations(pop_size,n_feat,mutation_rate,selection_rate, a, n_gen, from_gen = 0):\n",
    "    best_chromo= []\n",
    "    best_score= []\n",
    "    population_nextgen=initilization_of_population(pop_size,n_feat)\n",
    "    \n",
    "    #----------------------Ye maine change kiya------#\n",
    "    \n",
    "    scores = None\n",
    "    pop_after_fit = None\n",
    "    fitness_metrics = None\n",
    "    pop_after_sel = None\n",
    "    pop_after_cross = None\n",
    "\n",
    "    if from_gen != 0:\n",
    "        \n",
    "        print(f\"Need to load from prior run generation : {from_gen}\")\n",
    "        _generation_data = load_generation(gen_id = from_gen, constant = a)\n",
    "\n",
    "        scores = _generation_data[\"scores\"]\n",
    "        pop_after_fit = _generation_data[\"pop_after_fit\"]\n",
    "        fitness_metrics = _generation_data[\"fitness_metrics\"]\n",
    "        pop_after_sel = _generation_data[\"pop_after_sel\"]\n",
    "        pop_after_cross = _generation_data[\"pop_after_cross\"]\n",
    "        population_nextgen = _generation_data[\"population_nextgen\"]\n",
    "#       best_chromo = _generation_data[\"best_chromo\"]\n",
    "#       best_score = _generation_data[\"best_score\"]\n",
    "\n",
    "        from_gen += 1\n",
    "    #----------------------Yaha tak------#\n",
    "    \n",
    "    \n",
    "    for i in range(from_gen, n_gen):\n",
    "        scores, pop_after_fit, fitness_metrics = fitness_score(population_nextgen, generation = i, a = a)\n",
    "        print('Best score in generation',i,':',scores[:1])  #2\n",
    "        \n",
    "        pop_after_sel = selection(pop_after_fit,selection_rate,pop_size) \n",
    "        pop_after_cross = crossover(pop_after_sel,selection_rate,pop_size)\n",
    "        population_nextgen = mutation(pop_after_cross,mutation_rate,n_feat)\n",
    "        best_chromo.append(pop_after_fit[0])\n",
    "        best_score.append(scores[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"\\n==============================================================\\n\")\n",
    "        print(f\"saving generation : {i}\")\n",
    "        _generation_data = {\n",
    "            \"scores\": scores,\n",
    "            \"pop_after_fit\" : pop_after_fit,\n",
    "            \"fitness_metrics\" : fitness_metrics,\n",
    "            \"pop_after_sel\" : pop_after_sel,\n",
    "            \"pop_after_cross\" : pop_after_cross,\n",
    "            \"population_nextgen\" : population_nextgen,\n",
    "            \"best_chromo\" : pop_after_fit[0],\n",
    "            \"best_score\" : scores[0],   \n",
    "        }\n",
    "        \n",
    "        \n",
    "        save_generation(gen_id = i, constant = a, data = _generation_data)\n",
    "        print(f\"generation : {i} : constant : {a} saved successfully!\")\n",
    "        \n",
    "    \n",
    "        if gen_id in [7, 15, 31, 63, 95, 127]:\n",
    "            X_train_df, Y_train_df, X_test_df, Y_test_df = save_generated_feats(gen_id, score_bc, chromo_df_bc)\n",
    "            train_models(gen_id, X_train_df, Y_train_df, X_test_df, Y_test_df)\n",
    "            \n",
    "    return best_chromo,best_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2875249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing merit:\n",
      "Rcf:  0.032258453063504104 / Rff:  0.02199137718774412\n",
      "[09:54:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.032258494777117265 / Rff:  0.01994413477098124\n",
      "[09:55:22] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  4.200992949548073e-07 / Rff:  0.032358688961965795\n",
      "[09:56:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03225853895783199 / Rff:  0.035622264215184746\n",
      "[09:56:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03225859395527114 / Rff:  0.04083717251933548\n",
      "[09:57:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  4.793144648174613e-07 / Rff:  0.029669489265435212\n",
      "[09:58:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03225842583937622 / Rff:  0.033064713894106236\n",
      "[09:59:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03225844469209222 / Rff:  0.02532342812184894\n",
      "[09:59:46] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  5.512115261698258e-07 / Rff:  0.04771554277767175\n",
      "[10:00:29] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03225842955501857 / Rff:  0.038199891280446124\n",
      "[10:01:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best score in generation 0 : [0.4246686196101891]\n",
      "Crossover operation: \n",
      "Mutation operation: \n",
      "\n",
      "==============================================================\n",
      "\n",
      "saving generation : 0\n",
      "generation : 0 : constant : 0.4 saved successfully!\n",
      "computing merit:\n",
      "Rcf:  0.03571462302358203 / Rff:  0.02225611455713815\n",
      "[10:02:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03571479907137437 / Rff:  0.029968077401513023\n",
      "[10:02:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03333369925813959 / Rff:  0.033646937459681996\n",
      "[10:03:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04166696579583118 / Rff:  0.018441632167867227\n",
      "[10:03:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03125049542158088 / Rff:  0.04034739097609586\n",
      "[10:04:34] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.038462079982805084 / Rff:  0.03334519480694667\n",
      "[10:05:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.033333765222816454 / Rff:  0.025607920880119813\n",
      "[10:05:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  4.173891442029736e-07 / Rff:  0.029556540881750238\n",
      "[10:06:30] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03448321071192784 / Rff:  0.030637830601902113\n",
      "[10:07:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.043478637880225 / Rff:  0.0363781705267553\n",
      "[10:07:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best score in generation 1 : [0.4355177303856989]\n",
      "Crossover operation: \n",
      "Mutation operation: \n",
      "\n",
      "==============================================================\n",
      "\n",
      "saving generation : 1\n",
      "generation : 1 : constant : 0.4 saved successfully!\n",
      "computing merit:\n",
      "Rcf:  0.04000042491865084 / Rff:  0.014796106259975721\n",
      "[10:08:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03703735697859698 / Rff:  0.02822734815265537\n",
      "[10:08:58] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.05000038604428923 / Rff:  0.0395459200590752\n",
      "[10:09:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  5.213288871799084e-07 / Rff:  0.026164075661446986\n",
      "[10:10:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing merit:\n",
      "Rcf:  0.038462056099715376 / Rff:  0.02731964631652452\n",
      "[10:10:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.895693007661326e-07 / Rff:  0.04595866714239843\n",
      "[10:11:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04000050643765621 / Rff:  0.02206702168720399\n",
      "[10:11:58] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.037037557633421724 / Rff:  0.03701191745719093\n",
      "[10:12:30] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.0061652415725563e-07 / Rff:  0.06068049746338369\n",
      "[10:13:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.64553541617612e-07 / Rff:  0.035300225980948854\n",
      "[10:13:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best score in generation 2 : [0.44445588394504354]\n",
      "Crossover operation: \n",
      "Mutation operation: \n",
      "\n",
      "==============================================================\n",
      "\n",
      "saving generation : 2\n",
      "generation : 2 : constant : 0.4 saved successfully!\n",
      "computing merit:\n",
      "Rcf:  0.047619513677408666 / Rff:  0.042245471277446586\n",
      "[10:14:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03333369443829589 / Rff:  0.02318712567500317\n",
      "[10:15:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.038461806349939204 / Rff:  0.04720873229986942\n",
      "[10:15:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.038462001242712755 / Rff:  0.024529397453357308\n",
      "[10:16:16] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  4.896005882175334e-07 / Rff:  0.020002748801379814\n",
      "[10:16:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.035714789145223504 / Rff:  0.03528037259535022\n",
      "[10:17:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.93437340408653e-07 / Rff:  0.04615203413527649\n",
      "[10:18:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03448306244838052 / Rff:  0.03592222701565194\n",
      "[10:18:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04545473817160928 / Rff:  0.0439160601845405\n",
      "[10:19:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  2.75600111905119e-07 / Rff:  0.03610878033821552\n",
      "[10:19:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best score in generation 3 : [0.45652799824686197]\n",
      "Crossover operation: \n",
      "Mutation operation: \n",
      "\n",
      "==============================================================\n",
      "\n",
      "saving generation : 3\n",
      "generation : 3 : constant : 0.4 saved successfully!\n",
      "computing merit:\n",
      "Rcf:  0.04761915558820775 / Rff:  0.02558526329091196\n",
      "[10:20:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04545480590444762 / Rff:  0.02700238801414688\n",
      "[10:20:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.040000198337542 / Rff:  0.06852336915739282\n",
      "[10:21:30] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.0370374813834316 / Rff:  0.03169414548268049\n",
      "[10:22:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.05000055441417589 / Rff:  0.04720804585574502\n",
      "[10:22:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04000031023356085 / Rff:  0.04421165132411343\n",
      "[10:23:07] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03448313631241047 / Rff:  0.0185947024330826\n",
      "[10:23:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.0384618053545641 / Rff:  0.026554254034549078\n",
      "[10:24:27] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing merit:\n",
      "Rcf:  0.050000202479532796 / Rff:  0.02810371354713031\n",
      "[10:25:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04347852949171512 / Rff:  0.04257342624666336\n",
      "[10:25:37] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best score in generation 4 : [0.4628951265907327]\n",
      "Crossover operation: \n",
      "Mutation operation: \n",
      "\n",
      "==============================================================\n",
      "\n",
      "saving generation : 4\n",
      "generation : 4 : constant : 0.4 saved successfully!\n",
      "computing merit:\n",
      "Rcf:  0.050000127243880935 / Rff:  0.017034428847335605\n",
      "[10:26:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.258963554233942e-07 / Rff:  0.014163254613728717\n",
      "[10:26:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.343941496515369e-07 / Rff:  0.06231917138537363\n",
      "[10:27:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04347862055929919 / Rff:  0.040001151675284244\n",
      "[10:27:59] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03846186416501548 / Rff:  0.016155042506522985\n",
      "[10:28:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04545472561270762 / Rff:  0.05097085761509744\n",
      "[10:29:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.05000022180561818 / Rff:  0.053558575328909494\n",
      "[10:29:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04347869446407001 / Rff:  0.03413593328646372\n",
      "[10:30:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  2.2246667786455512e-07 / Rff:  0.023869638082118942\n",
      "[10:30:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04761939828784275 / Rff:  0.09201275527762035\n",
      "[10:31:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best score in generation 5 : [0.44246397569924256]\n",
      "Crossover operation: \n",
      "Mutation operation: \n",
      "\n",
      "==============================================================\n",
      "\n",
      "saving generation : 5\n",
      "generation : 5 : constant : 0.4 saved successfully!\n",
      "computing merit:\n",
      "Rcf:  3.836602523566662e-07 / Rff:  0.026074897972648146\n",
      "[10:31:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04000033937081829 / Rff:  0.02419880394610636\n",
      "[10:32:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.450085473531003e-07 / Rff:  0.09516333440794865\n",
      "[10:32:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04000024619197735 / Rff:  0.031154762469238467\n",
      "[10:33:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04545497101459308 / Rff:  0.0463899917384192\n",
      "[10:34:15] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04347846208359063 / Rff:  0.023502844314619922\n",
      "[10:35:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04545478092410276 / Rff:  0.03270756913048439\n",
      "[10:35:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  1.9092230084949919e-07 / Rff:  0.028562866585264562\n",
      "[10:36:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.040000294525879 / Rff:  0.02044850195849714\n",
      "[10:37:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  6.202068000649748e-07 / Rff:  0.06709855266192497\n",
      "[10:37:45] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best score in generation 6 : [0.45443783292873097]\n",
      "Crossover operation: \n",
      "Mutation operation: \n",
      "\n",
      "==============================================================\n",
      "\n",
      "saving generation : 6\n",
      "generation : 6 : constant : 0.4 saved successfully!\n",
      "computing merit:\n",
      "Rcf:  3.0349620940452975e-07 / Rff:  0.05586094077211072\n",
      "[10:38:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rcf:  0.041666996916256194 / Rff:  0.021141529895717326\n",
      "[10:39:06] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04000046550668605 / Rff:  0.04154380050581349\n",
      "[10:39:45] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04545475432393556 / Rff:  0.009224479932021452\n",
      "[10:40:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.043478483701738894 / Rff:  0.04421578420695991\n",
      "[10:40:55] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03571467447348594 / Rff:  0.02291911374847371\n",
      "[10:41:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  1.947925857469576e-07 / Rff:  0.05374071969491352\n",
      "[10:42:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  6.266007196239865e-07 / Rff:  0.08060162734664324\n",
      "[10:42:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04000035048032906 / Rff:  0.030388885599411844\n",
      "[10:43:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  5.20039542229842e-07 / Rff:  0.03917825966073742\n",
      "[10:43:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best score in generation 7 : [0.46351203465175084]\n",
      "Crossover operation: \n",
      "Mutation operation: \n",
      "\n",
      "==============================================================\n",
      "\n",
      "saving generation : 7\n",
      "generation : 7 : constant : 0.4 saved successfully!\n",
      "computing merit:\n",
      "Rcf:  3.0676036387573897e-07 / Rff:  0.022548648440171196\n",
      "[10:44:16] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.05000042862459738 / Rff:  0.035897785314743226\n",
      "[10:44:43] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.03846193033380417 / Rff:  0.031219269337534307\n",
      "[10:45:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04761934679961229 / Rff:  0.025732508250892083\n",
      "[10:45:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  4.3619528796201164e-07 / Rff:  0.0379745920195337\n",
      "[10:45:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04347872109531488 / Rff:  0.02992843029356485\n",
      "[10:46:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.680316349319241e-07 / Rff:  0.05378056371312927\n",
      "[10:46:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.2237663980350725e-07 / Rff:  0.031615210528944906\n",
      "[10:46:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04000032804240595 / Rff:  0.009787113324550801\n",
      "[10:46:59] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.05555603536123112 / Rff:  0.022184222380821718\n",
      "[10:47:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best score in generation 8 : [0.45570496432002683]\n",
      "Crossover operation: \n",
      "Mutation operation: \n",
      "\n",
      "==============================================================\n",
      "\n",
      "saving generation : 8\n",
      "generation : 8 : constant : 0.4 saved successfully!\n",
      "computing merit:\n",
      "Rcf:  0.04545487571874209 / Rff:  0.03903829892070537\n",
      "[10:47:33] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.05882393169011587 / Rff:  0.02410737750202488\n",
      "[10:47:55] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04166696561402817 / Rff:  0.00784877446528535\n",
      "[10:48:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04347877017651878 / Rff:  0.04822700498848983\n",
      "[10:48:30] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04000044104082691 / Rff:  0.03305065163048693\n",
      "[10:48:43] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing merit:\n",
      "Rcf:  0.0384619540182855 / Rff:  0.017550801520472215\n",
      "[10:48:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.816575576280056e-07 / Rff:  0.029673474540942782\n",
      "[10:49:13] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.151265307766672e-07 / Rff:  0.011050260875813305\n",
      "[10:49:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  0.04545483692782848 / Rff:  0.029314309447417676\n",
      "[10:50:00] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "computing merit:\n",
      "Rcf:  3.642321252176435e-07 / Rff:  0.044899043275266434\n",
      "[10:50:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best score in generation 9 : [0.45337184799429886]\n",
      "Crossover operation: \n",
      "Mutation operation: \n",
      "\n",
      "==============================================================\n",
      "\n",
      "saving generation : 9\n",
      "generation : 9 : constant : 0.4 saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "chromo_df_bc,score_bc = generations(pop_size=10,n_feat=X_train.shape[1],mutation_rate=0.20,selection_rate=0.8,a = 0.4, n_gen=128, from_gen=0)\n",
    "save_generated_feats(128, score_bc, chromo_df_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb4284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_generated_feats(gen_id, score_bc, chromo_df_bc):\n",
    "    ind = 0\n",
    "    l = len(score_bc)\n",
    "    _max = 0\n",
    "\n",
    "    for i in range(l):\n",
    "      if(_max < score_bc[i]):\n",
    "        _max = score_bc[i]\n",
    "        ind = i\n",
    "\n",
    "\n",
    "    chromosome = chromo_df_bc[ind]\n",
    "\n",
    "    print(chromosome)\n",
    "\n",
    "\n",
    "    X_train_df = X_train.iloc[:, chromosome]\n",
    "    X_test_df = X_test.iloc[:, chromosome]\n",
    "\n",
    "\n",
    "    Y_train_df = pd.DataFrame(Y_train, columns =['attack_cat'])\n",
    "    Y_test_df = pd.DataFrame(Y_test, columns = ['attack_cat'])\n",
    "\n",
    "    train_df = pd.concat([X_train_df, Y_train_df] , axis = 1)\n",
    "    test_df = pd.concat([X_test_df, Y_test_df] , axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_df.to_csv(f\"train_df_after_fs_{gen_id}.csv\",index=False)\n",
    "    test_df.to_csv(f\"test_df_after_fs_{gen_id}.csv\",index = False)\n",
    "    \n",
    "    return X_train_df, Y_train_df, X_test_df, Y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3089e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import modin.pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "%matplotlib inline \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from scipy.stats import pearsonr\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2,f_classif\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint\n",
    "%matplotlib inline \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import pickle\n",
    "from joblib import parallel_backend\n",
    "\n",
    "\n",
    "def save_gen_model_res(gen_id, data):\n",
    "    with open(f'gen_model_res_{str(gen_id)}_.pickle', 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_gen_model_res(gen_id) :    \n",
    "    with open(f'gen_model_res_{str(gen_id)}_.pickle', 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def DirectMetrics(actual, predicted):\n",
    "    \n",
    "    cr = classification_report(actual, predicted)\n",
    "    print(\"Classification_Report : \")\n",
    "    print(cr)\n",
    "  \n",
    "    Accuracy = accuracy_score(actual, predicted)\n",
    "    print(\"Accuracy: %.2f%%\" % (Accuracy * 100.0))\n",
    "    \n",
    "    Precision = precision_score(actual, predicted, average = 'macro')\n",
    "    Recall = recall_score(actual, predicted, average = 'macro')\n",
    "    F1_Score = f1_score(actual, predicted, average = 'macro')\n",
    "    \n",
    "    return {\"Direct_Precision\" : Precision,\n",
    "           \"Direct_Recall\" : Recall,\n",
    "           \"Direct_F1-Score\" : F1_Score}\n",
    "\n",
    "\n",
    "def ComputeMetrics(actual , predicted):\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    \n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "    \n",
    "    plt.figure(figsize = (9,9))\n",
    "    sns.heatmap(cm, annot = True, fmt = '0.3f', linewidth = 0.5, square = True, cbar = True)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('Actual Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    FP = cm.sum(axis = 0) - np.diag(cm)\n",
    "    FN = cm.sum(axis = 1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    avg_sensitivity = sum(sensitivity)/len(sensitivity)\n",
    "\n",
    "    specificity = TN/(TN+FP)\n",
    "    avg_specificity = sum(specificity)/len(specificity)\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "    avg_precision = sum(precision)/len(precision)\n",
    "    \n",
    "    recall = TP/(TP+FN)\n",
    "    avg_recall = sum(recall)/len(recall)\n",
    "    \n",
    "    f1_score = (2*recall*precision)/(recall + precision)\n",
    "    avg_f1_score = sum(f1_score)/len(f1_score)\n",
    "    \n",
    "    return {\"Sensitivity \" : avg_sensitivity,\n",
    "           \"Specificity \" : avg_specificity,\n",
    "           \"Precision \" : avg_precision,\n",
    "           \"Recall \" : avg_recall,\n",
    "           \"F1_Score \" : avg_f1_score}          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1049a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "    \n",
    "    \n",
    "def train_models(gen_id, X_train_df, Y_train_df, X_test_df, Y_test_df):\n",
    "\n",
    "    _dict = { model_type : {\"direct_metrics\" : None, \"metrics\" : None} for model_tye in [\"lr\", \"dt\", \"xgb\"]}\n",
    "\n",
    "    #Logistic Regression\n",
    "    model_lr = LogisticRegression()\n",
    "    model_lr.fit(X_train_df, Y_train_df)\n",
    "\n",
    "    Y_pred_lr = model_lr.predict(X_test_df)\n",
    "    predictions_lr = [round(value) for value in Y_pred]\n",
    "\n",
    "    direct_metrics_lr = DirectMetrics(Y_test_df, predictions_lr)\n",
    "    metrics_lr = ComputeMetrics(Y_test_df, predictions_lr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Decision Tree\n",
    "    model_dt = DecisionTreeClassifier()\n",
    "    model_dt.fit(X_train_df, Y_train_df)\n",
    "\n",
    "    Y_pred_dt = model.predict(X_test_df)\n",
    "    predictions_dt = [round(value) for value in Y_pred_dt]\n",
    "\n",
    "    direct_metrics_dt = DirectMetrics(Y_test_df, predictions_dt)\n",
    "    metrics_dt = ComputeMetrics(Y_test_df, predictions_dt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #XGBoost\n",
    "    model_xgb = XGBClassifier()\n",
    "    model_xgb.fit(X_train_df, Y_train_df)\n",
    "\n",
    "    Y_pred_xgb = model.predict(X_test_df)\n",
    "    predictions_xgb = [round(value) for value in Y_pred_xgb]\n",
    "\n",
    "    direct_metrics_xgb = DirectMetrics(Y_test_df, predictions_xgb)\n",
    "    metrics_xgb = ComputeMetrics(Y_test_df, predictions_xgb)\n",
    "\n",
    "\n",
    "    _dict[\"lr\"][\"direct_metrics\"] = direct_metrics_lr\n",
    "    _dict[\"lr\"][\"metrics\"] = metrics_lr\n",
    "\n",
    "    _dict[\"dt\"][\"direct_metrics\"] = direct_metrics_dt\n",
    "    _dict[\"dt\"][\"metrics\"] = metrics_dt\n",
    "\n",
    "    _dict[\"xgb\"][\"direct_metrics\"] = direct_metrics_xgb\n",
    "    _dict[\"xgb\"][\"metrics\"] = metrics_xb\n",
    "    \n",
    "    #save the values\n",
    "    save_gen_model_res(gen_id, _dict)\n",
    "    print(\"*\" * 50)\n",
    "    print(\"*\" * 50)\n",
    "    print(_dict)\n",
    "    print(\"*\" * 50)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75c549f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69d988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
