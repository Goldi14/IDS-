{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zfsfUCX7cUAP"},"outputs":[],"source":["import tensorflow as tf\n","tf.test.gpu_device_name()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ye0wLTvpRV9m"},"outputs":[],"source":["# module imports\n","import numpy as np\n","import pandas as pd\n","import math\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import itertools\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvwOZsPcRn7S"},"outputs":[],"source":["# model imports\n","import tensorflow as tf\n","import keras\n","# processing imports\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.preprocessing import LabelEncoder,OrdinalEncoder\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2,f_classif\n","from sklearn.  feature_selection import RFECV\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from scipy.stats import zscore"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37825,"status":"ok","timestamp":1675491077482,"user":{"displayName":"Arpita Srivastava","userId":"00636623162827386168"},"user_tz":-330},"id":"C-YZzV5lRpyz","outputId":"66412e6f-c603-4672-8f58-1c9cdcdf226e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["#Mounting google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvlapo4_R1Z1"},"outputs":[],"source":["# Reading datasets\n","test_df = pd.read_csv('/content/drive/MyDrive/NSLKDD_Arpita/NSL_KDD-master/KDDTest+.csv',header=None)\n","train_df = pd.read_csv('/content/drive/MyDrive/NSLKDD_Arpita/NSL_KDD-master/KDDTrain+.csv',header=None)\n","attackdf = pd.read_excel('/content/drive/MyDrive/NSLKDD_Arpita/NSL_KDD-master/Attack Types.xlsx',header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BR9Y_dfKvCRC"},"outputs":[],"source":["label_dict = {}\n","for row in attackdf.to_numpy():\n","  label_dict[row[0]] = row[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lS9CS8S1vtD4"},"outputs":[],"source":["print(label_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRN-ehpUSHWf"},"outputs":[],"source":["#column name\n","columns = ['duration'\n",",'protocol_type'\n",",'service'\n",",'flag'\n",",'src_bytes'\n",",'dst_bytes'\n",",'land'\n",",'wrong_fragment'\n",",'urgent'\n",",'hot'\n",",'num_failed_logins'\n",",'logged_in'\n",",'num_compromised'\n",",'root_shell'\n",",'su_attempted'\n",",'num_root'\n",",'num_file_creations'\n",",'num_shells'\n",",'num_access_files'\n",",'num_outbound_cmds'\n",",'is_host_login'\n",",'is_guest_login'\n",",'count'\n",",'srv_count'\n",",'serror_rate'\n",",'srv_serror_rate'\n",",'rerror_rate'\n",",'srv_rerror_rate'\n",",'same_srv_rate'\n",",'diff_srv_rate'\n",",'srv_diff_host_rate'\n",",'dst_host_count'\n",",'dst_host_srv_count'\n",",'dst_host_same_srv_rate'\n",",'dst_host_diff_srv_rate'\n",",'dst_host_same_src_port_rate'\n",",'dst_host_srv_diff_host_rate'\n",",'dst_host_serror_rate'\n",",'dst_host_srv_serror_rate'\n",",'dst_host_rerror_rate'\n",",'dst_host_srv_rerror_rate'\n",",'attack'\n",",'level']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bsV7nd2IS-YZ"},"outputs":[],"source":["train_df.columns = columns\n","test_df.columns = columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRuTEn3gTB84"},"outputs":[],"source":["trainsamples = train_df.shape[0]\n","testsamples = test_df.shape[0]\n","print(\"Training samples: \",trainsamples)\n","print(\"Testing samples: \",testsamples)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvdmuehTcIYD"},"outputs":[],"source":["train_df = pd.concat([train_df,test_df],axis = 0)\n","train_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5a_P4omyTPk-"},"outputs":[],"source":["train_df.reset_index(drop=True,inplace=True)\n","train_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_-zY49S8HwH"},"outputs":[],"source":["print(train_df.shape)\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5Hyfb0h0dDJ"},"outputs":[],"source":["# Map attack labels\n","train_df['attack'] = train_df['attack'].map(lambda x: label_dict[x])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dpbo9D7VEtpy"},"outputs":[],"source":["attack_s = train_df.iloc[:trainsamples,:]['attack']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvdSuAIZ5eu6"},"outputs":[],"source":["attack_s"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYEHvx7_TUgQ"},"outputs":[],"source":["# dataset statistics\n","train_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6RXwYyOUnzO"},"outputs":[],"source":["# count of Null values\n","train_df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xr5R0EZbUu6Z"},"outputs":[],"source":["# All attack labels\n","train_df['attack'].unique().tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffYl0IPzU_L8"},"outputs":[],"source":["# All protocl labels\n","train_df['protocol_type'].unique().tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVbVU-WAWDf1"},"outputs":[],"source":["# All flag labels\n","train_df['flag'].unique().tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHAjJFGXWPqd"},"outputs":[],"source":["# All service labels\n","train_df['service'].unique().tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xc4InWQC48fV"},"outputs":[],"source":["# count of attack labels\n","train_df[\"attack\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGLB9dwE-4mB"},"outputs":[],"source":["#Label encoded Dataset\n","\n","df = train_df.copy()\n","tolabelencode = ['protocol_type','service', 'flag']\n","lc = LabelEncoder()\n","for col in tolabelencode:\n","  df[col] = lc.fit_transform(df[col])\n","\n","#one hot encode labels\n","df = pd.get_dummies(df)\n","print(df.shape)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hVxcVoHlwVHo"},"outputs":[],"source":["labels = attack_s.unique()\n","NUM_LABELS = len(labels)\n","print(NUM_LABELS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AoQXvZpv94-"},"outputs":[],"source":["# Scaling data / Min-Max Scaling\n","\n","X = df.iloc[:,:df.shape[1]-NUM_LABELS]\n","Y = df.iloc[:,-NUM_LABELS:].to_numpy().astype(np.float32)\n","\n","sc = MinMaxScaler()\n","X = sc.fit_transform(X)\n","X = np.float32(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7SrBPyYwdL5"},"outputs":[],"source":["print(X.shape)\n","print(Y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTZrJSrawgvp"},"outputs":[],"source":["data = np.concatenate([X,Y],axis=1)\n","df = pd.DataFrame(data=data,columns = df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P29OPLW-w2nG"},"outputs":[],"source":["print(df.shape)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2x7RtVneA5Ag"},"outputs":[],"source":["#Split train and test dataset\n","print(\"Number of testing samples: \",test_df.shape[0])\n","test_df = df.iloc[-test_df.shape[0]:,:]\n","train_df = df.drop(index = df.iloc[-test_df.shape[0]:,:].index,axis = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlGTLRSMyF84"},"outputs":[],"source":["test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geRjGzUrB5wi"},"outputs":[],"source":["test_df.reset_index(inplace = True,drop=True)\n","test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vq2nCVtGCKYk"},"outputs":[],"source":["train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxZ_56a1ZMgI"},"outputs":[],"source":["#train_df.to_csv('/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/preprocessed_train_dataset.csv',index = False)"]},{"cell_type":"markdown","metadata":{"id":"ejywwX9hCtov"},"source":["################################################################################\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWU07PqbCNo3"},"outputs":[],"source":["print(\"Training samples: \",train_df.shape[0])\n","print(\"Testing samples: \",test_df.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXIcCFNwy0Lc"},"outputs":[],"source":["print(train_df.shape, test_df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82r4eYBVDTHO"},"outputs":[],"source":["def datasetbalancingUtility():\n","  attacks = attack_s.value_counts()\n","  max_count = attacks[0]\n","  major_class = []\n","  minor_class = []\n","  for name,count in zip(attacks.index,attacks):\n","    if(max_count - count >= max_count/2):\n","      minor_class.append([name,count])\n","    else:\n","      major_class.append([name,count])\n","  return major_class,minor_class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_qZV5jWDYWP"},"outputs":[],"source":["major_class,minor_class = datasetbalancingUtility()\n","print(major_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAGSt1Y7xshW"},"outputs":[],"source":["def generateTrainingDataset():\n","  res = []\n","  majclass = [x[0] for x in major_class]\n","  for label in majclass:\n","    index = attack_s[attack_s == label].index.to_list()\n","    res.extend(index)\n","  res.sort()\n","  return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-nmgGAmyOsY"},"outputs":[],"source":["indices = generateTrainingDataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vD19ZHBa8anz"},"outputs":[],"source":["mdf = train_df.drop(index = indices)\n","mdf.reset_index(inplace = True,drop = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgc8196f8sUW"},"outputs":[],"source":["print(mdf.shape)\n","mdf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aoUn847s_blh"},"outputs":[],"source":["X = mdf.iloc[:,:mdf.shape[1]-NUM_LABELS]\n","Y = mdf.iloc[:,-NUM_LABELS:].to_numpy().astype(np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAM20TNZ14Q3"},"outputs":[],"source":["print(X.shape)\n","print(Y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTnYM5Ruc1Vo"},"outputs":[],"source":["BATCH_SIZE = 128\n","dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n","dataset = dataset.shuffle(buffer_size=512).batch(BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"htm4LEVKlejx"},"source":["# Conditional WGAN-GP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtIz2BmXoWLg"},"outputs":[],"source":["# Importing Libraries\n","\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","from matplotlib import pyplot as plt\n","import numpy as np\n","\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EQqxkGtPbBW"},"outputs":[],"source":["# parameters\n","LAYERS_DIM = 128\n","OUTPUT_DIM = X.shape[1]\n","Z_NOISE_DIM = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fRU5-sFFNKW"},"outputs":[],"source":["def Generator():\n","    model = Sequential(name=\"generator\")\n","    model.add(layers.Input(shape=(Z_NOISE_DIM + NUM_LABELS,)))\n","    model.add(layers.Dense(128,activation='relu'))\n","    model.add(layers.Dense(64,activation='relu'))\n","    model.add(layers.Dense(OUTPUT_DIM,activation='sigmoid'))\n","    return model\n","\n","def Discriminator():\n","    model = Sequential(name=\"critic\")\n","    model.add(layers.Input(shape=(OUTPUT_DIM + NUM_LABELS,)))\n","    model.add(layers.Dense(128,activation='relu'))\n","    model.add(layers.Dense(64,activation='relu'))\n","    model.add(layers.Dense(1,activation='linear'))\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71DepywXy0Ns"},"outputs":[],"source":["generator = Generator()\n","generator.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cO76v1OWQp4a"},"outputs":[],"source":["critic = Discriminator()\n","critic.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RZkBgcyRQej"},"outputs":[],"source":["class WGAN_GP(keras.Model):\n","    def __init__(self,\n","                 critic,\n","                 generator,\n","                 latent_dim,\n","                 critic_extra_steps,\n","                 gp_weight=10.0): # UPDATE for WGAN-GP: gradient penalty weight\n","        super().__init__()\n","        self.critic = critic\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","        self.c_extra_steps = critic_extra_steps\n","        self.gp_weight = gp_weight # WGAN-GP\n","        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n","        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n","        self.d_lossArray = []\n","        self.g_lossArray = []\n","\n","\n","    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n","        super(WGAN_GP, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.d_loss_fn = d_loss_fn\n","        self.g_loss_fn = g_loss_fn\n","\n","    @property\n","    def metrics(self):\n","        return [self.d_loss_metric, self.g_loss_metric]\n","\n","    # UPDATE for WGAN-GP: use gradient penalty instead of weight clipping\n","    def gradient_penalty(self, batch_size, real_images, fake_images):\n","        \"\"\" Calculates the gradient penalty.\n","\n","        Gradient penalty is calculated on an interpolated image\n","        and added to the discriminator loss.\n","        \"\"\"\n","        alpha = tf.random.normal([batch_size, 1], 0.0, 1.0)\n","        diff = fake_images - real_images\n","        # 1. Create the interpolated image\n","        interpolated = real_images + alpha * diff\n","\n","        with tf.GradientTape() as gp_tape:\n","            gp_tape.watch(interpolated)\n","            # 2. Get the Critic's output for the interpolated image\n","            pred = self.critic(interpolated, training=True)\n","\n","        # 3. Calculate the gradients w.r.t to the interpolated image\n","        grads = gp_tape.gradient(pred, [interpolated])[0]\n","        # 4. Calculate the norm of the gradients.\n","        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))\n","        # 5. Calculate gradient penalty\n","        gradient_penalty = tf.reduce_mean((norm - 1.0) ** 2)\n","        return gradient_penalty\n","\n","    def train_step(self, data):\n","\n","        real_packet, one_hot_labels = data\n","\n","        batch_size = tf.shape(real_packet)[0]\n","        noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        real_packet = tf.concat([real_packet,one_hot_labels],axis=1)\n","        noise_sample = tf.concat([noise,one_hot_labels],axis=1)\n","\n","        # Train the critic more often than the generator by 5 times (self.c_extra_steps)\n","        for i in range(self.c_extra_steps):\n","            # Step 1. Train the critic with both real images and fake images\n","            with tf.GradientTape() as tape:\n","                pred_real = self.critic(real_packet, training=True)\n","                fake_packet = self.generator(noise_sample, training=True)\n","                fake_packet = tf.concat([fake_packet,one_hot_labels],axis=1)\n","                pred_fake = self.critic(fake_packet, training=True)\n","                # UPDATE for WGAN-GP: Calculate the gradient penalty\n","                gp = self.gradient_penalty(batch_size, real_packet, fake_packet)\n","                # UPDATE for WGAN-GP: Add gradient penalty to the original critic loss\n","                d_loss = self.d_loss_fn(pred_real, pred_fake) + gp * self.gp_weight\n","            # Compute critic gradients\n","            grads = tape.gradient(d_loss, self.critic.trainable_variables)\n","            # Update critic weights\n","            self.d_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n","\n","        # Step 2. Train the generator (do not update weights of the critic)\n","        misleading_labels = tf.ones((batch_size, 1)) # G wants D to think the fake images are real (label as 1)\n","\n","        with tf.GradientTape() as tape:\n","            fake_packet = self.generator(noise_sample, training=True)\n","            fake_packet = tf.concat([fake_packet,one_hot_labels],axis=1)\n","            pred_fake = self.critic(fake_packet, training=True)\n","            g_loss = self.g_loss_fn(pred_fake)\n","        # Compute generator gradients\n","        grads = tape.gradient(g_loss, self.generator.trainable_variables)\n","        # Update generator wieghts\n","        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\n","\n","        self.d_loss_metric.update_state(d_loss)\n","        self.g_loss_metric.update_state(g_loss)\n","\n","        return {\"d_loss\": self.d_loss_metric.result(), \"g_loss\": self.g_loss_metric.result()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z79JSxLGdclV"},"outputs":[],"source":["class GANMonitor(keras.callbacks.Callback):\n","    def on_train_end(self, logs=None):\n","        self.model.generator.save('/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/final_model_generator.h5')\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.d_lossArray.append(self.model.d_loss_metric.result().numpy())\n","        self.model.g_lossArray.append(self.model.g_loss_metric.result().numpy())\n","        if epoch % 8 == 0:\n","          self.model.generator.save(f'/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/generator_{epoch}.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9CMzYFW_dpOE"},"outputs":[],"source":["wgan_gp = WGAN_GP(critic=critic,\n","              generator=generator,\n","              latent_dim=Z_NOISE_DIM,\n","              critic_extra_steps=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzvxnu5zdrsE"},"outputs":[],"source":["# Wasserstein loss for the critic\n","def d_wasserstein_loss(pred_real, pred_fake):\n","    real_loss = tf.reduce_mean(pred_real)\n","    fake_loss = tf.reduce_mean(pred_fake)\n","    return fake_loss - real_loss\n","\n","# Wasserstein loss for the generator\n","def g_wasserstein_loss(pred_fake):\n","    return -tf.reduce_mean(pred_fake)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3CTGNjLcd0wf"},"outputs":[],"source":["LR = 0.0002 # WGAN-GP paper recommends lr of 0.0002\n","d_optimizer = keras.optimizers.Adam(learning_rate=LR, beta_1=0.5, beta_2=0.9) # UPDATE for WGAN-GP: use Adam instead of RMSProp\n","g_optimizer = keras.optimizers.Adam(learning_rate=LR, beta_1=0.5, beta_2=0.9) # UPDATE for WGAN-GP: use Adam instead of RMSProp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8OAkZ_Td2m6"},"outputs":[],"source":["wgan_gp.compile(\n","    d_optimizer=d_optimizer,\n","    g_optimizer=g_optimizer,\n","    d_loss_fn = d_wasserstein_loss,\n","    g_loss_fn = g_wasserstein_loss\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twulj-P7d4Fk"},"outputs":[],"source":["NUM_EPOCHS = 256 # number of epochs\n","wgan_gp.fit(dataset, epochs=NUM_EPOCHS, callbacks=[GANMonitor()])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5298,"status":"ok","timestamp":1675493323799,"user":{"displayName":"Arpita Srivastava","userId":"00636623162827386168"},"user_tz":-330},"id":"c72j1XWZzdAf","outputId":"382f8079-4209-454f-9fa5-b3c722d7743c"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["from keras.models import load_model\n","model = load_model('/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/final_model_generator.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VZUI_Cx7klp"},"outputs":[],"source":["#trained_generator = wgan_gp.generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2M-xp03eaBHa"},"outputs":[],"source":["# Critic loss Graph Plot\n","plt.figure(figsize=(7,4))\n","plt.plot(wgan_gp.d_lossArray, label='Critic loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Critic loss')\n","plt.legend()\n","plt.grid()\n","plt.savefig('/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/critic_loss.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9eZM9mWzrUIX"},"outputs":[],"source":["# Generator loss Graph Plot\n","plt.figure(figsize=(7,4))\n","plt.plot(wgan_gp.g_lossArray, label='Generator loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Generator loss')\n","plt.legend()\n","plt.grid()\n","plt.savefig('/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/generator_loss.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yHeTEc8rgXm"},"outputs":[],"source":["# Critic & Generator loss Graph Plot\n","plt.figure(figsize=(7,4))\n","plt.plot(wgan_gp.d_lossArray, label='Critic loss')\n","plt.plot(wgan_gp.g_lossArray, label='Generator loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid()\n","plt.savefig('/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/critic_&_generator_loss.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_ZApIaCnczZ"},"outputs":[],"source":["Loading trained model\n","trained_generator = tf.keras.models.load_model('/content/generator.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdW_vFQ5HXhm"},"outputs":[],"source":["test_df.to_csv('/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/preprocessed_test_dataset.csv',index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rV_5b7nxLt0R"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"mqj9c3mvLPJD"},"source":["# Dataset Balancing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCPaI2vP-tNI"},"outputs":[],"source":["def BuildLabelMapping():\n","  labels.sort()\n","  label_mapping = {}\n","  for i in range(0,len(labels)):\n","    label_mapping[labels[i]] = i\n","  return label_mapping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijAdHSaY_fQu"},"outputs":[],"source":["label_mapping = BuildLabelMapping()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abb8WzyyjPwW"},"outputs":[],"source":["print(label_mapping)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcOR7_WS5Ztr"},"outputs":[],"source":["print(minor_class, major_class)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSV4lyxX5eZP"},"outputs":[],"source":["major_class[-1][1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AxPk6NVwpVtO"},"outputs":[],"source":["def generate_sample(no_of_samples, attack_label):\n","    arr = np.zeros(shape=(no_of_samples,NUM_LABELS))\n","    idx = label_mapping.get(attack_label)\n","    if(idx != None):\n","      for row in arr:\n","        row[label_mapping[attack_label]] = 1\n","\n","    noise = np.random.normal(size=(no_of_samples, Z_NOISE_DIM))\n","    noise_sample = np.concatenate((noise,arr),axis=1)\n","    generated_sample = model.predict(noise_sample)\n","    return generated_sample,arr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1G3RoBZM4obp"},"outputs":[],"source":["for minclass in minor_class:\n","    noOfSample = major_class[-1][1] - minclass[1]\n","    print(f\"major_class : {major_class[-1][1]}\")\n","    print(f\"minclass : {minclass[1]}\")\n","    print(f\"noOfSample : {noOfSample}\")\n","    print(\"*\" * 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBKZx0GO9Hym"},"outputs":[],"source":["def generateSamples2(pct):\n","  samples_generated = []\n","  for minclass in minor_class:\n","    noOfSample = major_class[-1][1] - minclass[1]\n","    noOfSample = int(noOfSample * pct) + 1\n","    print(f\"major_class : {major_class[-1][1]}\")\n","    print(f\"minclass : {minclass[1]}\")\n","    print(f\"noOfSample : {noOfSample}\")\n","    print(\"*\" * 50)\n","    res,label = generate_sample(noOfSample,minclass[0])\n","    res = np.concatenate((res,label),axis=1)\n","    for row in res:\n","      samples_generated.append(row)\n","  return samples_generated\n","\n","\n","for pct in [0, 0.0001, 0.001, 0.01, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n","    samples = generateSamples2(pct = pct)\n","\n","    balanced_dataset = pd.DataFrame(data = samples,columns = df.columns)\n","\n","    dataset = pd.concat([train_df,balanced_dataset])\n","    dataset.reset_index(inplace=True,drop=True)\n","\n","    #Saving the dataframe\n","    dataset.to_csv(f'/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/Gan_generated_train_dataset/balanced_train_dataset_{str(pct)}.csv',index=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfgC5CbtLIRY"},"outputs":[],"source":["pip install modin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnn40UAz7b2H"},"outputs":[],"source":["import numpy as np\n","import modin.pandas as pd\n","import sklearn\n","\n","from xgboost import XGBClassifier\n","\n","import xgboost as xgb\n","\n","\n","import numpy as np\n","\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def LabelAttack(Y):\n","  attack = []\n","  for i in range(Y.shape[0]):\n","    k = 0\n","    for j in range(Y.shape[1]):\n","      if(Y[i][j] == 1):\n","        k = j\n","        break\n","    attack.append(k)\n","  return attack\n","\n","\n","def get_train_test_data(pct):\n","    test_df = pd.read_csv('/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/preprocessed_test_dataset.csv')                 #.sample(1024)\n","    train_df = pd.read_csv(f'/content/drive/MyDrive/NSLKDD_Arpita/Downloaded_Files/Gan_generated_train_dataset/balanced_train_dataset_{str(pct)}.csv')      #.sample(1024)\n","\n","    print(train_df.shape, test_df.shape)\n","\n","    label = ['dos', 'normal', 'probe', 'r2l', 'u2r']\n","   # label = ['Analysis','Backdoor','DoS','Exploits','Fuzzers','Generic','Normal','Reconnaissance','Shellcode','Worms']\n","\n","    X_train = train_df.iloc[:,:train_df.shape[1]-5]\n","    Y_train = train_df.iloc[:,-5:].to_numpy()\n","\n","    X_test = test_df.iloc[:,:test_df.shape[1]-5]\n","    Y_test = test_df.iloc[:,-5:].to_numpy()\n","\n","\n","    Y_train = np.asarray(LabelAttack(Y_train)).reshape(-1,1)\n","    Y_test = np.asarray(LabelAttack(Y_test)).reshape(-1,1)\n","\n","\n","    return X_train, Y_train, X_test, Y_test\n","\n","\n","\n","def DirectMetrics(actual, predicted):\n","\n","    cr = classification_report(actual, predicted)\n","    print(\"Classification_Report : \")\n","    print(cr)\n","\n","    Accuracy = accuracy_score(actual, predicted)\n","    print(\"Accuracy: %.2f%%\" % (Accuracy * 100.0))\n","\n","    Precision = precision_score(actual, predicted, average = 'weighted')\n","    Recall = recall_score(actual, predicted, average = 'weighted')\n","    F1_Score = f1_score(actual, predicted, average = 'weighted')\n","\n","    return {\"Direct_Precision\" : Precision,\n","           \"Direct_Recall\" : Recall,\n","           \"Direct_F1-Score\" : F1_Score}\n","\n","\n","def ComputeMetrics(actual , predicted):\n","    TP, TN, FP, FN = 0, 0, 0, 0\n","\n","    cm = confusion_matrix(actual, predicted)\n","\n","\n","    FP = cm.sum(axis = 0) - np.diag(cm)\n","    FN = cm.sum(axis = 1) - np.diag(cm)\n","    TP = np.diag(cm)\n","    TN = cm.sum() - (FP + FN + TP)\n","\n","    FP = FP.astype(float)\n","    FN = FN.astype(float)\n","    TP = TP.astype(float)\n","    TN = TN.astype(float)\n","\n","    sensitivity = TP/(TP+FN)\n","    avg_sensitivity = sum(sensitivity)/len(sensitivity)\n","\n","    specificity = TN/(TN+FP)\n","    avg_specificity = sum(specificity)/len(specificity)\n","\n","    precision = TP/(TP+FP)\n","    avg_precision = sum(precision)/len(precision)\n","\n","    recall = TP/(TP+FN)\n","    avg_recall = sum(recall)/len(recall)\n","\n","    f1_score = (2*recall*precision)/(recall + precision)\n","    avg_f1_score = sum(f1_score)/len(f1_score)\n","\n","    FAR = FP/(FP+TN)\n","    avg_FAR = sum(FAR)/len(FAR)\n","\n","    return {\"Sensitivity \" : avg_sensitivity,\n","           \"Specificity \" : avg_specificity,\n","           \"Precision \" : avg_precision,\n","           \"Recall \" : avg_recall,\n","           \"F1_Score \" : avg_f1_score,\n","           \"FAR\" : avg_FAR}\n","\n","xgb_params = {\n","    \"booster\": \"dart\",\n","    \"verbosity\": 0,\n","    \"objective\": \"multi:softmax\",\n","    \"num_class\" : 10,\n","    \"lambda\": 1.234568712743763e-06,\n","    \"alpha\": 0.021824183515918392,\n","    \"subsample\": 0.7966629501270384,\n","    \"colsample_bytree\": 0.8575214799710436,\n","    \"early_stopping_rounds\": 24,\n","    \"n_estimators\": 32,\n","    \"max_depth\": 7,\n","    \"min_child_weight\": 5,\n","    \"eta\": 0.020721025441133932,\n","    \"gamma\": 8.632145831151602e-05,\n","    \"grow_policy\": \"depthwise\",\n","    \"sample_type\": \"uniform\",\n","    \"normalize_type\": \"forest\",\n","    \"rate_drop\": 1.4263688272813651e-08,\n","    \"skip_drop\": 3.191224113185437e-05,\n","    #\"n_jobs\" : -1\n","}\n","\n","\n","def train_models(X_train_df, Y_train_df, X_test_df, Y_test_df):\n","\n","    _dict = { model_type : {\"direct_metrics\" : None, \"metrics\" : None} for model_type in [\"xgb\"]}\n","\n","\n","    #----------------xgb model-------------------#\n","    dtrain = xgb.DMatrix(X_train_df, label=Y_train_df)\n","    dvalid = xgb.DMatrix(X_test_df, label=Y_test_df)\n","\n","    model = xgb.train(xgb_params, dtrain)\n","    preds = model.predict(dvalid)\n","    predictions_xgb = np.rint(preds)\n","    acc = sklearn.metrics.accuracy_score(Y_test_df, predictions_xgb)\n","\n","    direct_metrics_xgb = DirectMetrics(Y_test_df, predictions_xgb)\n","    metrics_xgb = ComputeMetrics(Y_test_df, predictions_xgb)\n","\n","\n","    _dict[\"xgb\"][\"direct_metrics\"] = direct_metrics_xgb\n","    _dict[\"xgb\"][\"metrics\"] = metrics_xgb\n","\n","\n","    print(\"*\" * 50)\n","    print(\"*\" * 50)\n","    print(_dict)\n","    print(\"*\" * 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"26MV-Gry7u3i"},"outputs":[],"source":["for pct in [0, 0.0001, 0.001, 0.01, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]:\n","    print(f\"testing for {pct}\")\n","    print(\"*\" * 50)\n","    X_train_df, Y_train_df, X_test_df, Y_test_df = get_train_test_data(pct)\n","    train_models(X_train_df, Y_train_df, X_test_df, Y_test_df)\n","    print(\"_\" * 50)\n","    print(\"*\" * 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynBrHCFC8MBb"},"outputs":[],"source":["###############################################################################################################################################"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}